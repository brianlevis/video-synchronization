<!DOCTYPE html>
<html>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
    <head>
        <title>CS 194-26 Final Project</title>
        <style>body { font-family: 'Open Sans', sans-serif; }</style>
    </head>
    <body>
        <h1>CS 194-26 Final Project: Synchronizing Videos to Music</h1>
        <h3>Overview</h3>
        <p>My goal for this project was to alter the progression of video frames to match an audio accompaniment.</p>
        <h3>Sample Results</h3>
        <p>(in the form of copyright-infringing videos)</p>
        <iframe width=640 height=360 src="https://drive.google.com/file/d/1cSHLuvPqO2U4EsIztz0oldGbmmb-sMRA/preview"></iframe>
        <p>A ballerina dances to changing music.</p>
        <iframe width=640 height=360 src="https://drive.google.com/file/d/1fc32oY7rCBnoyHHfrc0N9RAR1uYEqpw3/preview"></iframe>
        <p>A montage of clips - ballet, soldiers, children, and a flock of dogs.</p>
        <a href="https://github.com/brianlevis/video-synchronization">View, download, and run code here.</a>
        <h2>Technical Explanation</h2>
        <p><a href="http://www.abedavis.com/files/papers/VisualRhythm_Davis18.pdf">This paper</a> from Abe Davis aims to accomplish a similar task. I found that my results looked a little better when I implemented the match-candidate detection from this paper, accounting for pitch changes and directional motion. This paper additionally creates tempograms to locally align audio and video rhythms, and has some <a href="http://abedavis.com/visualbeat/">pretty neat results.</a></p>
        <div align="middle">
            <iframe width=1708 height=480 src="https://drive.google.com/file/d/1W_dVXmfuHDTkFpOdbJv6vrlcpZ0uDO4E/preview"></iframe>
            <p>The Wiggles show off their flashy car to the instrumental of REDMERCEDES.</p>
        </div>
        <h3>Dimensionality Reduction for Music</h3>
        <p>Compute the spectogram (complex, time-valued FFT) of the mono-audio signal. The spectral flux between time steps indicates change in both pitch and volume. Sum the positive frequency values across their axis to get a vector of beat-onset envelopes.</p>
        <div align="middle">
            <table>
                <tr>
                    <td>
                        <img src="https://brianlevis.com/cs194-26/final/res/audio_signal.png" align="middle" height="550px"/>
                        <figcaption align="middle">audio signal from above, with match points</figcaption>
                    </td>
                </tr>
                <tr>
                    <td>
                        <img src="https://brianlevis.com/cs194-26/final/res/spectrogram.png" align="middle" height="550px"/>
                        <figcaption align="middle">spectrogram</figcaption>
                    </td>
                    <td>
                        <img src="https://brianlevis.com/cs194-26/final/res/spectral_flux.png" align="middle" height="550px"/>
                        <figcaption align="middle">spectral flux</figcaption>
                    </td>
                </tr>
                <tr>
                    <td>
                        <img src="https://brianlevis.com/cs194-26/final/res/onset_envelopes.png" align="middle" height="550px"/>
                        <figcaption align="middle">onset envelopes</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <h3>Dimensionality Reduction for Video</h3>
        <p>The process for video is analogous. Replace frequency with angles, and frequency strength with the magnitude of motion in that direction. Motion for each angle is created by binning and summing optical flow vectors for every pixel.
        Sum positive magnitudes to compute deceleration, apply a median filter to account for duplicated frames, and remove outliers that may indicate transitions to obtain impact envelopes.</p>
        <div align="middle">
            <table>
                <tr>
                    <td>
                        <img src="https://brianlevis.com/cs194-26/final/res/video_signal.png" align="middle" height="550px"/>
                        <figcaption align="middle">video signal from above, with match points</figcaption>
                    </td>
                </tr>
                <tr>
                    <td>
                        <img src="https://brianlevis.com/cs194-26/final/res/directogram.png" align="middle" height="550px"/>
                        <figcaption align="middle">directogram</figcaption>
                    </td>
                    <td>
                        <img src="https://brianlevis.com/cs194-26/final/res/deceleration.png" align="middle" height="550px"/>
                        <figcaption align="middle">deceleration</figcaption>
                    </td>
                </tr>
                <tr>
                    <td>
                        <img src="https://brianlevis.com/cs194-26/final/res/impact_envelopes.png" align="middle" height="550px"/>
                        <figcaption align="middle">impact envelopes</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <h3>Aligning Media</h3>
        <p>Matching points are computed for both media types as a simple local maximum. The minimum distance between peaks is a constant factor of the sample rate.
        I found that peak detection on the impact envelopes yielded similar results to running peak detection with optical flow magnitude and using the falling edge.</p>
        <div align="middle">
            <table>
                <tr>
                    <td>
                        <img src="https://brianlevis.com/cs194-26/final/res/warp_curve.png" align="middle" height="550px"/>
                        <figcaption align="middle">warp curve from the paper referenced above</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <p>Match points were iteratively matched together, and constrained by bounds on video speed change. Audio was resized as necessary. Once match points were found, each section of video was reinterpolated and aligned with the audio. My frames were equally spaced temporally, but Abe Davis spaced frames using a 3rd-degree polynomial fitted to match times.</p>
        <h2>Summary</h2>
        <p>I have reached a result I am happy with, but I will continue to experiment with tweaks, and mostly try to copy stuff that Abe does.</p>
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-105387166-2"></script>
        <script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config','UA-105387166-2');</script>
    </body>
</html>
